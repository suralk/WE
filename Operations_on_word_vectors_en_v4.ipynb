{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Operations_on_word_vectors_en_v4.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"diVTUwrRApuT","colab_type":"code","outputId":"c5f3d797-1414-4859-ee33-6d16b9641670","colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["!pip3 install pyfasttext"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pyfasttext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/ef/90606442481d1e4ab10eba8c2b2c449ceaa70c60e9b8d5898bb7504e3634/pyfasttext-0.4.6.tar.gz (244kB)\n","\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyfasttext) (0.16.0)\n","Requirement already satisfied: cysignals in /usr/local/lib/python3.6/dist-packages (from pyfasttext) (1.10.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyfasttext) (1.16.4)\n","Requirement already satisfied: Cython>=0.28 in /usr/local/lib/python3.6/dist-packages (from cysignals->pyfasttext) (0.29.10)\n","Building wheels for collected packages: pyfasttext\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UD5Xv9fg1xh9","colab_type":"code","colab":{}},"source":["from pyfasttext import FastText\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKq6GaG3U6ky","colab_type":"code","outputId":"ffc1ed47-121e-4a46-f690-92f77fab218b","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1562304266973,"user_tz":-330,"elapsed":818,"user":{"displayName":"Ishadi Jayasinghe","photoUrl":"https://lh4.googleusercontent.com/-aTeOb_ukgaI/AAAAAAAAAAI/AAAAAAAAA7M/q5cfQs7zlws/s64/photo.jpg","userId":"15884559479470787819"}}},"source":["# Run this cell to mount your Google Drive.\n","from google.colab import drive\n","import sys\n","drive.mount('/content/drive')\n","sys.path.insert(0, 'drive/My Drive/WordEmbed workshop/')\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sYf4kvor3AAJ","colab_type":"code","outputId":"9e064089-a77f-4a91-95d4-0e907ed8d728","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1562304272819,"user_tz":-330,"elapsed":3208,"user":{"displayName":"Ishadi Jayasinghe","photoUrl":"https://lh4.googleusercontent.com/-aTeOb_ukgaI/AAAAAAAAAAI/AAAAAAAAA7M/q5cfQs7zlws/s64/photo.jpg","userId":"15884559479470787819"}}},"source":["from utils import *\n","from w2v_utils import *"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"G92FT34s2yHp","colab_type":"text"},"source":["# Operations on word vectors\n","\n","Welcome to your first activity. \n","\n","Because word embeddings are very computionally expensive to train, most ML practitioners will load a pre-trained set of embeddings. \n","\n","**After this assignment you will be able to:**\n","\n","- Load pre-trained word vectors, and measure similarity using cosine similarity\n","- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.  \n","\n","Let's get started! Run the following cell to load pre-trained word vectors (would take a couple of minutes to load)"]},{"cell_type":"code","metadata":{"id":"z0k2-6FY2U1n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"e4838d36-1dfc-4434-e62f-02747c4d368c","executionInfo":{"status":"error","timestamp":1562304329028,"user_tz":-330,"elapsed":817,"user":{"displayName":"Ishadi Jayasinghe","photoUrl":"https://lh4.googleusercontent.com/-aTeOb_ukgaI/AAAAAAAAAAI/AAAAAAAAA7M/q5cfQs7zlws/s64/photo.jpg","userId":"15884559479470787819"}}},"source":["embeddings_index = FastText(\"drive/My Drive/WordEmbed workshop/cc.en.300.bin\") "],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-07d5d4f72760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/WordEmbed workshop/cc.en.300.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'FastText' is not defined"]}]},{"cell_type":"code","metadata":{"id":"mr_K2mGwBOr0","colab_type":"code","outputId":"b592cfac-f129-4e7c-b989-cab3cb386afb","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["#play with different index values\n","list(embeddings_index.words)[:10], embeddings_index['the'],len( embeddings_index['the'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([',', 'the', '.', 'and', 'to', 'of', 'a', '</s>', 'in', 'is'],\n"," array('f', [-0.05174419283866882, 0.0739639550447464, -0.013056879863142967, 0.04472655802965164, -0.03432036563754082, 0.02121688425540924, 0.006911486387252808, -0.01632784679532051, -0.01807485707104206, -0.0019965237006545067, -0.10204669088125229, 0.005904885940253735, 0.025654055178165436, -0.002596620935946703, -0.05855605751276016, -0.037758685648441315, 0.016311872750520706, 0.014632370322942734, -0.008759298361837864, -0.01759478449821472, -0.008547326549887657, -0.007793376222252846, -0.018278032541275024, 0.008798242546617985, 0.0013020262122154236, -0.09382941573858261, 0.013899145647883415, 0.014892999082803726, -0.03937097638845444, -0.029441121965646744, 0.009422930888831615, -0.02522841840982437, -0.010441077873110771, -0.22131945192813873, -0.022859765216708183, -0.008935268968343735, -0.032222650945186615, 0.08217015862464905, 0.0020999780390411615, 0.028173504397273064, 0.007170667871832848, -0.00912560522556305, -0.035169392824172974, -0.017804421484470367, -0.07055401802062988, 0.06302309036254883, -0.00924630742520094, -0.022327037528157234, -0.005585512146353722, 0.05147229880094528, -0.03069112077355385, 0.04364822804927826, -0.010969555005431175, -0.05545424297451973, 0.00893828459084034, -0.0672699511051178, 0.010507602244615555, 0.05740974843502045, 0.009920522570610046, -0.02826792560517788, 0.04704095795750618, 0.005292295478284359, 0.0030449405312538147, 0.0007154792547225952, 0.04429377615451813, 0.006895273923873901, -0.03340554237365723, 0.009057371877133846, -0.007582707330584526, 0.006601395085453987, 0.09174107015132904, 0.03111150674521923, 0.05429111048579216, 0.02817249670624733, -0.019965246319770813, -0.0333779975771904, 0.005287552252411842, 0.03638041019439697, 0.2249329686164856, 0.09276068955659866, -0.012265386059880257, 0.00856030359864235, -0.059897832572460175, 0.06762705743312836, 0.04024453088641167, 0.0011667766375467181, 0.04639219492673874, -0.04369712620973587, 0.005942209158092737, 0.09172087162733078, -0.04124822840094566, -0.015125337988138199, -0.023081663995981216, 0.009499152190983295, 0.05883144959807396, 0.02786044403910637, 0.06469924747943878, -0.05675431713461876, -0.01295602135360241, 0.04743509739637375, 0.03537055850028992, -0.012107647955417633, -0.0076668281108140945, -0.1306004673242569, 0.01344328559935093, -0.0505647212266922, 0.01107924897223711, 0.011873988434672356, -0.022108890116214752, 0.039386481046676636, 0.022054724395275116, 0.024534281343221664, 0.003912844695150852, 0.11459276080131531, 0.022843660786747932, -0.04676138609647751, -0.045948576182127, -0.01894749328494072, 0.007636964786797762, -0.03021598979830742, -0.03481895849108696, -0.028876017779111862, -0.039914265275001526, 0.024452757090330124, -0.010191294364631176, 0.05778583884239197, -0.03878986835479736, -0.011682924814522266, -0.030445951968431473, 0.24659813940525055, -0.011076081544160843, 0.035597313195466995, 0.004620131105184555, 0.2094080150127411, -0.10221105813980103, 0.03359135985374451, 0.06883871555328369, -0.07079383730888367, 0.026868334040045738, -0.04231182485818863, 0.007686006836593151, -0.026735538616776466, 0.007240753620862961, 0.0034507745876908302, 0.035066086798906326, -0.006336905062198639, -0.44622430205345154, 0.010489501059055328, -0.012010100297629833, -0.044891420751810074, -0.16962137818336487, 0.0503978431224823, 0.09296766668558121, -0.0043746852315962315, -0.004063506610691547, 0.03217855095863342, 0.20264926552772522, 0.06125467270612717, -0.029505934566259384, 0.022754304111003876, -0.019002921879291534, 0.017326589673757553, 0.14805170893669128, -0.017467476427555084, -0.012527287937700748, 0.06865523755550385, 0.03330358862876892, -0.030255811288952827, 0.04277052730321884, 0.005125343333929777, 0.022832775488495827, 0.010362190194427967, 0.07314387708902359, 0.007851246744394302, -0.005077663343399763, 0.05432640761137009, -0.032466545701026917, 0.05114980787038803, 0.02881649136543274, -0.05859564244747162, -6.183981895446777e-06, 0.04932169243693352, 0.016600094735622406, -0.014284498989582062, 0.03588688373565674, 0.054288849234580994, -0.0004584691487252712, -0.05893496423959732, 0.016184048727154732, -0.02216803841292858, -0.01989128813147545, 0.023476550355553627, -0.06784828007221222, 0.017855927348136902, 0.0032871775329113007, 0.011379102244973183, 0.04730954393744469, -0.0443134680390358, 0.032302889972925186, 0.019490331411361694, -0.0647159069776535, 0.3388279676437378, 0.06986933946609497, -0.021548673510551453, -0.024406395852565765, -0.0033775940537452698, -0.003429962322115898, -0.062153734266757965, 0.012268307618796825, 0.037522654980421066, -0.01969302073121071, 0.02413400635123253, -0.08767689019441605, 0.020063111558556557, -0.006146462634205818, -0.02558313123881817, -0.01914634183049202, -0.02643878199160099, 0.0190159659832716, -0.042245060205459595, 0.025072818621993065, 0.08248689025640488, -0.009623252786695957, 0.12875349819660187, 0.06209681183099747, 0.05381625518202782, 0.01886638067662716, 0.04219500347971916, 0.1802852302789688, -0.0010247062891721725, -0.032816626131534576, -0.055915482342243195, -0.015692859888076782, 0.0489620566368103, 0.03521936386823654, -0.04174818843603134, 0.01590695232152939, -0.0766218826174736, -0.06572385877370834, 0.04971179738640785, 0.01023605465888977, 0.1471206694841385, -0.07105539739131927, -0.1468748301267624, 0.4736766219139099, -0.016923142597079277, -0.005086713936179876, 0.0158682893961668, 0.05499573424458504, -0.06344765424728394, -0.021004319190979004, 0.012220926582813263, 0.026895735412836075, 0.005955509841442108, 0.06639760732650757, 0.0106058269739151, -0.07053034752607346, -0.020743293687701225, -0.07843244075775146, -0.029104094952344894, -0.028336700052022934, -0.15680161118507385, -0.03934377431869507, 0.005044909194111824, 0.020447611808776855, -0.0025592390447854996, 0.043642982840538025, 0.027916643768548965, -0.03925796225667, 0.03671174496412277, -0.00417267344892025, -0.01563604734838009, -0.07332682609558105, -0.1636732965707779, 0.06523250043392181, -0.0062313321977853775, -0.06500299274921417, -0.1984354555606842, -0.04105719178915024, -0.15339209139347076, 0.00200753565877676, 0.013305407017469406, -0.23640002310276031, -0.05279095470905304, -0.00420718640089035, -0.04465389624238014, 0.01120448112487793, -0.033242981880903244, -0.05501563847064972, 0.0013315193355083466, 0.016918648034334183, -0.0438513346016407, -0.05778319388628006, 0.022305436432361603, -0.07770661264657974, -0.04319952428340912, -0.025055358186364174, 0.2370251715183258, 0.00044751912355422974, -0.004193066619336605]),\n"," 300)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"oNoHyTet2yH4","colab_type":"text"},"source":["# 1 - Cosine similarity\n","\n","To measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n","\n","$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n","\n","where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. \n","\n","<img src=\"https://drive.google.com/uc?id=16VOhhx1BuX5bPpWAH_L5mRbCS3IqJCwf\">\n","<caption><center> **Figure 1**: The cosine of the angle between two vectors is a measure of how similar they are</center></caption>\n","\n","The function `cosine_similarity()` is implemented for you."]},{"cell_type":"code","metadata":{"id":"EabTLOtS2yH6","colab_type":"code","colab":{}},"source":["def cosine_similarity(u, v):\n","    \"\"\"\n","    Cosine similarity reflects the degree of similariy between u and v\n","        \n","    Arguments:\n","        u -- a word vector of shape (n,)          \n","        v -- a word vector of shape (n,)\n","\n","    Returns:\n","        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n","    \"\"\"\n","    \n","    distance = 0.0\n","    \n","    # Compute the dot product between u and v (≈1 line)\n","    dot = np.dot(u, v)\n","    # Compute the L2 norm of u (≈1 line)\n","    norm_u =np.linalg.norm(u)\n","#     norm_u = np.sqrt(np.sum(u * u))\n","    \n","    # Compute the L2 norm of v (≈1 line)\n","    norm_v = np.linalg.norm(v)\n","    # Compute the cosine similarity defined by formula (1) (≈1 line)\n","    cosine_similarity = dot / (norm_u * norm_v)\n","        \n","    return cosine_similarity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lh0E0JnGSYXP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnqOCVe52yH9","colab_type":"code","outputId":"3ef1ca97-698b-4f2f-d25a-514b4fd5e7e3","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# some similarity values are hard-coded here. You can change them and try your own ones\n","father = embeddings_index[\"father\"]\n","mother = embeddings_index[\"mother\"]\n","ball = embeddings_index[\"ball\"]\n","crocodile = embeddings_index[\"crocodile\"]\n","france = embeddings_index[\"france\"]\n","italy = embeddings_index[\"italy\"]\n","paris = embeddings_index[\"paris\"]\n","rome = embeddings_index[\"rome\"]\n","\n","\n","print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n","print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n","print(\"cosine_similarity(france - paris, rome - italy) = \"\n","      ,cosine_similarity(np.subtract(france, paris), np.subtract(rome, italy)))\n","\n","king = embeddings_index[\"king\"]\n","queen = embeddings_index[\"queen\"]\n","man = embeddings_index[\"man\"]\n","woman = embeddings_index[\"woman\"]\n","\n","print(\"cosine_similarity(france - paris, rome - italy) = \"\n","      ,cosine_similarity(np.subtract(king, man), np.subtract(queen, woman)))\n","\n","raven = embeddings_index[\"raven\"]\n","nevermore = embeddings_index[\"nevermore\"]\n","print(\"cosine_similarity(raven, nevermore) = \", cosine_similarity(raven, nevermore))\n","\n","pig = embeddings_index[\"pig\"]\n","oink = embeddings_index[\"oink\"]\n","print(\"cosine_similarity(pig, oink) = \", cosine_similarity(pig, oink))\n","\n","print(\"cosine_similarity(oink - pig, nevermore - raven) = \"\n","      ,cosine_similarity(np.subtract(oink, pig), np.subtract(nevermore, raven)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cosine_similarity(father, mother) =  0.83168906\n","cosine_similarity(ball, crocodile) =  0.16411223\n","cosine_similarity(france - paris, rome - italy) =  -0.26101252\n","cosine_similarity(france - paris, rome - italy) =  0.75547177\n","cosine_similarity(raven, nevermore) =  0.5220859\n","cosine_similarity(pig, oink) =  0.57947254\n","cosine_similarity(oink - pig, nevermore - raven) =  0.29101586\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NZVTFaUq2yIF","colab_type":"text"},"source":["## 2 - Word analogy task\n","\n","In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. In detail, we are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. "]},{"cell_type":"code","metadata":{"id":"CA4ZjfYg2yIG","colab_type":"code","colab":{}},"source":["def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n","    \"\"\"\n","    Performs the word analogy task as explained above: a is to b as c is to ____. \n","    \n","    Arguments:\n","    word_a -- a word, string\n","    word_b -- a word, string\n","    word_c -- a word, string\n","    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n","    \n","    Returns:\n","    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n","    \"\"\"\n","    \n","    # convert words to lower case\n","    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n","    \n","    \n","    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n","    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n","    \n","    \n","#     words = word_to_vec_map.words\n","    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n","    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n","\n","    # loop over the whole word vector set\n","    for w in word_to_vec_map.words:        \n","        # to avoid best_word being one of the input words, pass on them.\n","        if w in [word_a, word_b, word_c] :\n","            continue\n","        \n","        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n","        cosine_sim = cosine_similarity(np.subtract(e_b, e_a),\n","                                       np.subtract(word_to_vec_map[w], e_c))\n","        \n","        # If the cosine_sim is more than the max_cosine_sim seen so far,\n","            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n","        if cosine_sim > max_cosine_sim:\n","            max_cosine_sim = cosine_sim\n","            best_word = w        \n","        \n","    return best_word"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsDjgpt32yIJ","colab_type":"text"},"source":["Run the cell below to test your code, this may take 1-2 minutes."]},{"cell_type":"code","metadata":{"id":"_NNKo0Yg2yIL","colab_type":"code","outputId":"42b7c77f-21e9-421f-a80e-ee97101f677b","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n","for triad in triads_to_try:\n","    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, embeddings_index)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["italy -> italian :: spain -> spanish\n","india -> delhi :: japan -> delhi.\n","man -> woman :: boy -> girl\n","small -> smaller :: large -> larger\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IThqiXcp2yIP","colab_type":"text"},"source":["**Expected Output**:\n","\n","<table>\n","    <tr>\n","        <td>\n","            **italy -> italian** ::\n","        </td>\n","        <td>\n","         spain -> spanish\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **india -> delhi** ::\n","        </td>\n","        <td>\n","         japan -> tokyo\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **man -> woman ** ::\n","        </td>\n","        <td>\n","         boy -> girl\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **small -> smaller ** ::\n","        </td>\n","        <td>\n","         large -> larger\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"dDi4BAK82yIQ","colab_type":"text"},"source":["Once you get the correct expected output, please feel free to modify the input cells above to test your own analogies. Try to find some other analogy pairs that do work, but also find some where the algorithm doesn't give the right answer: For example, you can try small->smaller as big->?.  "]},{"cell_type":"markdown","metadata":{"id":"wNUZ9db12yIS","colab_type":"text"},"source":["### Congratulations!\n","\n","You've come to the end of this task. Here are the main points you should remember:\n","\n","- Cosine similarity a good way to compare similarity between pairs of word vectors. (Though L2 distance works too.) \n","- For NLP applications, using a pre-trained set of word vectors from the internet is often a good way to get started. "]}]}